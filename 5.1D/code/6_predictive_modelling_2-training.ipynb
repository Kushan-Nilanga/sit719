{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "def train_and_save(pipelines, dataset_name, x, y):\n",
    "    try:\n",
    "        results = load_json(\n",
    "            f'../artefacts/6/results_{dataset_name}.json')\n",
    "    except:\n",
    "        results = {}\n",
    "\n",
    "    for model_name, params, pipeline in pipelines:\n",
    "        start = time.time()\n",
    "        print(f\"Training {model_name}\", end=' -> ')\n",
    "        pipeline.set_params(**params.get('best_parameters'))\n",
    "        pipeline.fit(x, y)\n",
    "        print(f\"Saving {model_name}\", end=' -> ')\n",
    "        score = pipeline.score(x.iloc[:10000], y.iloc[:10000])\n",
    "        print(f\"Score: {score}\")\n",
    "        joblib.dump(\n",
    "            pipeline, f'../artefacts/6/models_{dataset_name}_{model_name}.joblib')\n",
    "        end = time.time()\n",
    "\n",
    "        try:\n",
    "            results[model_name]['train'] = {\n",
    "                'time': end - start,\n",
    "                'score': score\n",
    "            }\n",
    "        except:\n",
    "            results[model_name] = {\n",
    "                'train': {\n",
    "                    'time': end - start,\n",
    "                    'score': score\n",
    "                }\n",
    "            }\n",
    "\n",
    "        with open(f'../artefacts/6/results_{dataset_name}.json', 'w+') as f:\n",
    "            json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ada_boost -> Saving ada_boost -> Score: 0.9904\n",
      "Training decision_tree -> Saving decision_tree -> Score: 0.9986\n",
      "Training knn -> Saving knn -> Score: 0.9999\n",
      "Training logistic_regression -> Saving logistic_regression -> Score: 0.96\n",
      "Training mlp -> Saving mlp -> Score: 0.9878\n",
      "Training random_forest -> Saving random_forest -> Score: 0.9962\n",
      "Training svm -> Saving svm -> Score: 0.9971\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# kdd\n",
    "\n",
    "verbose = False\n",
    "\n",
    "kdd_pipelines = [\n",
    "    (\n",
    "        \"ada_boost\",\n",
    "        load_json('../artefacts/6/params_kdd_ada_boost.json'),\n",
    "        Pipeline(\n",
    "            [\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', OneVsRestClassifier(\n",
    "                    AdaBoostClassifier(), verbose=verbose, n_jobs=-1))\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"decision_tree\",\n",
    "        load_json('../artefacts/6/params_kdd_decision_tree.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', DecisionTreeClassifier())\n",
    "        ])\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"knn\",\n",
    "        load_json('../artefacts/6/params_kdd_knn.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', KNeighborsClassifier(n_jobs=-1))\n",
    "        ])\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"logistic_regression\",\n",
    "        load_json('../artefacts/6/params_kdd_logistic_regression.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', OneVsRestClassifier(\n",
    "                    LogisticRegression(n_jobs=-1, verbose=verbose), verbose=verbose, n_jobs=-1))\n",
    "        ])\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"mlp\",\n",
    "        load_json('../artefacts/6/params_kdd_mlp.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', MLPClassifier(verbose=verbose))\n",
    "        ])\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"random_forest\",\n",
    "        load_json('../artefacts/6/params_kdd_random_forest.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', RandomForestClassifier(verbose=verbose, n_jobs=-1))\n",
    "        ])\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"svm\",\n",
    "        load_json('../artefacts/6/params_kdd_svm.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', OneVsRestClassifier(\n",
    "                    SVC(verbose=verbose), verbose=verbose, n_jobs=-1))\n",
    "        ])\n",
    "    )\n",
    "\n",
    "]\n",
    "\n",
    "df_kdd = pd.read_csv('../artefacts/5/kdd_train.csv', index_col=0)\n",
    "\n",
    "kdd_x = df_kdd.iloc[:, :-3]\n",
    "kdd_y = df_kdd.iloc[:, -3:]\n",
    "\n",
    "train_and_save(kdd_pipelines, 'kdd', kdd_x, kdd_y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training UNSW Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ada_boost -> Saving ada_boost -> Score: 0.9808\n",
      "Training decision_tree -> Saving decision_tree -> Score: 0.9943\n",
      "Training knn -> Saving knn -> Score: 1.0\n",
      "Training logistic_regression -> "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donkushanathalage/Desktop/sit719/env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/donkushanathalage/Desktop/sit719/env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logistic_regression -> Score: 0.9829\n",
      "Training mlp -> Saving mlp -> Score: 0.9862\n",
      "Training random_forest -> Saving random_forest -> Score: 0.9963\n",
      "Training svm -> "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [43], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m unsw_x \u001b[39m=\u001b[39m df_unsw\u001b[39m.\u001b[39miloc[:, :\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m     77\u001b[0m unsw_y \u001b[39m=\u001b[39m df_unsw\u001b[39m.\u001b[39miloc[:, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[0;32m---> 79\u001b[0m train_and_save(unsw_pipelines, \u001b[39m'\u001b[39;49m\u001b[39munsw\u001b[39;49m\u001b[39m'\u001b[39;49m, unsw_x, unsw_y)\n",
      "Cell \u001b[0;32mIn [41], line 17\u001b[0m, in \u001b[0;36mtrain_and_save\u001b[0;34m(pipelines, dataset_name, x, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m -> \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m pipeline\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbest_parameters\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> 17\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(x, y)\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSaving \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m -> \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m score \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mscore(x\u001b[39m.\u001b[39miloc[:\u001b[39m10000\u001b[39m], y\u001b[39m.\u001b[39miloc[:\u001b[39m10000\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/sit719/env/lib/python3.10/site-packages/sklearn/pipeline.py:382\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    381\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 382\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/sit719/env/lib/python3.10/site-packages/sklearn/multiclass.py:327\u001b[0m, in \u001b[0;36mOneVsRestClassifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    323\u001b[0m columns \u001b[39m=\u001b[39m (col\u001b[39m.\u001b[39mtoarray()\u001b[39m.\u001b[39mravel() \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m Y\u001b[39m.\u001b[39mT)\n\u001b[1;32m    324\u001b[0m \u001b[39m# In cases where individual estimators are very fast to train setting\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[39m# of spawning threads.  See joblib issue #112.\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose)(\n\u001b[1;32m    328\u001b[0m     delayed(_fit_binary)(\n\u001b[1;32m    329\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator,\n\u001b[1;32m    330\u001b[0m         X,\n\u001b[1;32m    331\u001b[0m         column,\n\u001b[1;32m    332\u001b[0m         classes\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    333\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mnot \u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_binarizer_\u001b[39m.\u001b[39;49mclasses_[i],\n\u001b[1;32m    334\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_binarizer_\u001b[39m.\u001b[39;49mclasses_[i],\n\u001b[1;32m    335\u001b[0m         ],\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    337\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, column \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(columns)\n\u001b[1;32m    338\u001b[0m )\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mn_features_in_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_features_in_\n",
      "File \u001b[0;32m~/Desktop/sit719/env/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/Desktop/sit719/env/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/Desktop/sit719/env/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "\n",
    "unsw_pipelines = [\n",
    "    (\n",
    "        \"ada_boost\",\n",
    "        load_json('../artefacts/6/params_unsw_ada_boost.json'),\n",
    "        Pipeline(\n",
    "            [\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', OneVsRestClassifier(\n",
    "                    AdaBoostClassifier(), verbose=verbose, n_jobs=-1))\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"decision_tree\",\n",
    "        load_json('../artefacts/6/params_unsw_decision_tree.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', DecisionTreeClassifier())\n",
    "        ])\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"knn\",\n",
    "        load_json('../artefacts/6/params_unsw_knn.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', KNeighborsClassifier(n_jobs=-1))\n",
    "        ])\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"logistic_regression\",\n",
    "        load_json('../artefacts/6/params_unsw_logistic_regression.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', OneVsRestClassifier(\n",
    "                    LogisticRegression(n_jobs=-1, verbose=verbose), verbose=verbose, n_jobs=-1))\n",
    "        ])\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"mlp\",\n",
    "        load_json('../artefacts/6/params_unsw_mlp.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', MLPClassifier(verbose=verbose))\n",
    "        ])\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"random_forest\",\n",
    "        load_json('../artefacts/6/params_unsw_random_forest.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', RandomForestClassifier(verbose=verbose, n_jobs=-1))\n",
    "        ])\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"svm\",\n",
    "        load_json('../artefacts/6/params_unsw_svm.json'),\n",
    "        Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('clf', OneVsRestClassifier(\n",
    "                    SVC(verbose=verbose), verbose=verbose, n_jobs=-1))\n",
    "        ])\n",
    "    )\n",
    "\n",
    "]\n",
    "\n",
    "df_unsw = pd.read_csv('../artefacts/5/unsw_train.csv', index_col=0)\n",
    "\n",
    "unsw_x = df_unsw.iloc[:, :-2]\n",
    "unsw_y = df_unsw.iloc[:, -2:]\n",
    "\n",
    "train_and_save(unsw_pipelines, 'unsw', unsw_x, unsw_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c4ca806edb215e68744eb731c739c4121ba4017c72f37ff523d6d3cfcbd411d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
